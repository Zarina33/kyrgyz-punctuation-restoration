{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mBERT Baseline for Kyrgyz Punctuation Restoration\n",
    "\n",
    "This notebook trains `bert-base-multilingual-cased` on the Kyrgyz punctuation restoration task as a baseline comparison for the XLM-RoBERTa model.\n",
    "\n",
    "**Before running:** Go to Runtime → Change runtime type → select **T4 GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets seqeval accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload dataset\n",
    "\n",
    "Upload your `train_data.json` file when prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Upload train_data.json\n",
    "if not os.path.exists('train_data.json'):\n",
    "    uploaded = files.upload()\n",
    "    print(\"File uploaded!\")\n",
    "else:\n",
    "    print(\"File already exists.\")\n",
    "\n",
    "with open('train_data.json', 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"Total sentences: {len(raw_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare dataset for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "LABEL2ID = {'O': 0, 'COMMA': 1, 'PERIOD': 2, 'QUESTION': 3}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "NUM_LABELS = len(LABEL2ID)\n",
    "\n",
    "def extract_tokens_and_labels(text):\n",
    "    \"\"\"Extract (word, label) pairs from punctuated text.\"\"\"\n",
    "    raw_tokens = text.split()\n",
    "    words = []\n",
    "    labels = []\n",
    "    for token in raw_tokens:\n",
    "        word = token.rstrip('.,?!;:\\'\\\"\\u201c\\u201d\\u2013\\u2014\\u2026()[]{}\\u00ab\\u00bb')\n",
    "        if not word:\n",
    "            continue\n",
    "        trailing = token[len(word):]\n",
    "        label = 'O'\n",
    "        for char in trailing:\n",
    "            if char == '.':\n",
    "                label = 'PERIOD'\n",
    "                break\n",
    "            elif char == ',':\n",
    "                label = 'COMMA'\n",
    "                break\n",
    "            elif char == '?':\n",
    "                label = 'QUESTION'\n",
    "                break\n",
    "        words.append(word)\n",
    "        labels.append(label)\n",
    "    return words, labels\n",
    "\n",
    "# Process all sentences\n",
    "all_words = []\n",
    "all_labels = []\n",
    "skipped = 0\n",
    "\n",
    "for entry in raw_data:\n",
    "    text = entry['text'][0]\n",
    "    words, labels = extract_tokens_and_labels(text)\n",
    "    if len(words) < 2:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    all_words.append(words)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "print(f\"Processed: {len(all_words)} sentences, skipped: {skipped}\")\n",
    "\n",
    "# Label distribution\n",
    "flat_labels = [l for labels in all_labels for l in labels]\n",
    "print(f\"Label distribution: {Counter(flat_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_words, test_words, train_labels, test_labels = train_test_split(\n",
    "    all_words, all_labels, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "train_words, val_words, train_labels, val_labels = train_test_split(\n",
    "    train_words, train_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_words)}, Val: {len(val_words)}, Test: {len(test_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenize with mBERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "MAX_LEN = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class PunctDataset(Dataset):\n",
    "    def __init__(self, words_list, labels_list, tokenizer, max_len):\n",
    "        self.words_list = words_list\n",
    "        self.labels_list = labels_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words = self.words_list[idx]\n",
    "        labels = self.labels_list[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        label_ids = []\n",
    "        previous_word_id = None\n",
    "\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word_id:\n",
    "                label_ids.append(LABEL2ID[labels[word_id]])\n",
    "            else:\n",
    "                # For sub-tokens, only the last subtoken gets the label\n",
    "                label_ids.append(-100)\n",
    "            previous_word_id = word_id\n",
    "\n",
    "        # Fix: assign label to LAST subtoken of each word instead of first\n",
    "        label_ids_fixed = [-100] * len(word_ids)\n",
    "        for i in range(len(word_ids) - 1, -1, -1):\n",
    "            wid = word_ids[i]\n",
    "            if wid is None:\n",
    "                continue\n",
    "            if i == len(word_ids) - 1 or word_ids[i + 1] != wid:\n",
    "                # This is the last subtoken for this word\n",
    "                label_ids_fixed[i] = LABEL2ID[labels[wid]]\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label_ids_fixed, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = PunctDataset(train_words, train_labels, tokenizer, MAX_LEN)\n",
    "val_dataset = PunctDataset(val_words, val_labels, tokenizer, MAX_LEN)\n",
    "test_dataset = PunctDataset(test_words, test_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "print(f\"Datasets created. Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Quick sanity check\n",
    "sample = train_dataset[0]\n",
    "print(f\"Input shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "print(f\"Non-ignored labels: {(sample['labels'] != -100).sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=NUM_LABELS,\n    id2label=ID2LABEL,\n    label2id=LABEL2ID,\n)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n\n    # Flatten and remove ignored tokens\n    true_labels = []\n    pred_labels = []\n    for i in range(labels.shape[0]):\n        for j in range(labels.shape[1]):\n            if labels[i][j] != -100:\n                true_labels.append(ID2LABEL[labels[i][j]])\n                pred_labels.append(ID2LABEL[preds[i][j]])\n\n    report = classification_report(\n        true_labels, pred_labels,\n        labels=['O', 'COMMA', 'PERIOD', 'QUESTION'],\n        output_dict=True\n    )\n\n    return {\n        'f1_weighted': report['weighted avg']['f1-score'],\n        'precision_weighted': report['weighted avg']['precision'],\n        'recall_weighted': report['weighted avg']['recall'],\n    }\n\ntraining_args = TrainingArguments(\n    output_dir='./mbert_punct_kyrgyz',\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='f1_weighted',\n    greater_is_better=True,\n    fp16=True,\n    logging_steps=50,\n    report_to='none',\n    seed=42,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"Starting training...\")\ntrainer.train()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Get predictions on test set\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "all_true = []\n",
    "all_pred = []\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels']\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu()\n",
    "\n",
    "        for i in range(labels.shape[0]):\n",
    "            for j in range(labels.shape[1]):\n",
    "                if labels[i][j] != -100:\n",
    "                    all_true.append(ID2LABEL[labels[i][j].item()])\n",
    "                    all_pred.append(ID2LABEL[preds[i][j].item()])\n",
    "\n",
    "print(f\"Total test tokens evaluated: {len(all_true)}\")\n",
    "print()\n",
    "\n",
    "# Full classification report\n",
    "print(\"=\" * 60)\n",
    "print(\"mBERT CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(\n",
    "    all_true, all_pred,\n",
    "    labels=['O', 'COMMA', 'PERIOD', 'QUESTION'],\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results summary\n",
    "\n",
    "Copy the **weighted avg** Precision, Recall, F1 from the report above.\n",
    "\n",
    "These numbers go into the paper's Table 3 (main results):\n",
    "\n",
    "```\n",
    "| Model              | Precision | Recall | F1    |\n",
    "|--------------------|-----------|--------|-------|\n",
    "| Rule-based         | 0.801     | 0.781  | 0.790 |\n",
    "| mBERT              | ???       | ???    | ???   |  <-- YOUR NUMBERS HERE\n",
    "| XLM-RoBERTa (ours) | 0.941     | 0.868  | 0.903 |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print in a copy-paste friendly format\n",
    "report = classification_report(\n",
    "    all_true, all_pred,\n",
    "    labels=['O', 'COMMA', 'PERIOD', 'QUESTION'],\n",
    "    digits=3,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "w = report['weighted avg']\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COPY THESE NUMBERS FOR THE PAPER:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"mBERT  &  {w['precision']:.3f}  &  {w['recall']:.3f}  &  {w['f1-score']:.3f}\")\n",
    "print()\n",
    "print(\"LaTeX table row:\")\n",
    "print(f\"mBERT  & {w['precision']:.3f} & {w['recall']:.3f} & {w['f1-score']:.3f} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Confusion Matrix",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nlabels = ['O', 'COMMA', 'PERIOD', 'QUESTION']\n\n# Normalized confusion matrix\ncm_norm = confusion_matrix(all_true, all_pred, labels=labels, normalize='true')\n\nfig, ax = plt.subplots(figsize=(6, 5))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=labels)\ndisp.plot(ax=ax, cmap='Blues', values_format='.3f')\nax.set_title('mBERT — Confusion Matrix (Normalized)', fontsize=13)\nplt.tight_layout()\nplt.savefig('confusion_matrix_mbert.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\"Saved: confusion_matrix_mbert.png\")\n\n# Raw counts\ncm_raw = confusion_matrix(all_true, all_pred, labels=labels)\nprint(\"\\nRaw counts:\")\nprint(cm_raw)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "files.download('confusion_matrix_mbert.png')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Save Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import shutil\n\nSAVE_DIR = './mbert_kyrgyz_punct_model'\n\n# Save model and tokenizer\nmodel.save_pretrained(SAVE_DIR)\ntokenizer.save_pretrained(SAVE_DIR)\n\nprint(f\"Model and tokenizer saved to: {SAVE_DIR}\")\nprint(f\"Files:\")\nfor f in sorted(os.listdir(SAVE_DIR)):\n    size = os.path.getsize(os.path.join(SAVE_DIR, f))\n    print(f\"  {f} ({size / 1e6:.1f} MB)\" if size > 1e6 else f\"  {f} ({size} B)\")\n\n# Zip for download\nshutil.make_archive(SAVE_DIR, 'zip', SAVE_DIR)\nprint(f\"\\nArchive created: {SAVE_DIR}.zip\")\nfiles.download(f'{SAVE_DIR}.zip')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}