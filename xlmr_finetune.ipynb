{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM-RoBERTa Fine-tuning for Kyrgyz Punctuation Restoration\n\nFine-tunes `xlm-roberta-base` on the Kyrgyz punctuation dataset.\n\n**Same split & setup as mBERT** for fair comparison.\n\n**Before running:** Runtime → Change runtime type → **T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q transformers datasets seqeval accelerate scikit-learn"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import files\nimport json\nimport os\n\nif not os.path.exists('train_data.json'):\n    uploaded = files.upload()\n    print(\"File uploaded!\")\nelse:\n    print(\"File already exists.\")\n\nwith open('train_data.json', 'r') as f:\n    raw_data = json.load(f)\n\nprint(f\"Total sentences: {len(raw_data)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\nfrom collections import Counter\n\nLABEL2ID = {'O': 0, 'COMMA': 1, 'PERIOD': 2, 'QUESTION': 3}\nID2LABEL = {v: k for k, v in LABEL2ID.items()}\nNUM_LABELS = len(LABEL2ID)\n\ndef extract_tokens_and_labels(text):\n    raw_tokens = text.split()\n    words = []\n    labels = []\n    for token in raw_tokens:\n        word = token.rstrip('.,?!;:\\'\\\"\\u201c\\u201d\\u2013\\u2014\\u2026()[]{}\\u00ab\\u00bb')\n        if not word:\n            continue\n        trailing = token[len(word):]\n        label = 'O'\n        for char in trailing:\n            if char == '.':\n                label = 'PERIOD'\n                break\n            elif char == ',':\n                label = 'COMMA'\n                break\n            elif char == '?':\n                label = 'QUESTION'\n                break\n        words.append(word)\n        labels.append(label)\n    return words, labels\n\nall_words = []\nall_labels = []\nskipped = 0\n\nfor entry in raw_data:\n    text = entry['text'][0]\n    words, labels = extract_tokens_and_labels(text)\n    if len(words) < 2:\n        skipped += 1\n        continue\n    all_words.append(words)\n    all_labels.append(labels)\n\nprint(f\"Processed: {len(all_words)} sentences, skipped: {skipped}\")\nflat_labels = [l for labels in all_labels for l in labels]\nprint(f\"Label distribution: {Counter(flat_labels)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n\n# SAME split as mBERT for fair comparison\ntrain_words, test_words, train_labels, test_labels = train_test_split(\n    all_words, all_labels, test_size=0.15, random_state=42\n)\n\ntrain_words, val_words, train_labels, val_labels = train_test_split(\n    train_words, train_labels, test_size=0.1, random_state=42\n)\n\nprint(f\"Train: {len(train_words)}, Val: {len(val_words)}, Test: {len(test_words)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer\nimport torch\nfrom torch.utils.data import Dataset\n\nMODEL_NAME = \"xlm-roberta-base\"\nMAX_LEN = 256\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nclass PunctDataset(Dataset):\n    def __init__(self, words_list, labels_list, tokenizer, max_len):\n        self.words_list = words_list\n        self.labels_list = labels_list\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.words_list)\n\n    def __getitem__(self, idx):\n        words = self.words_list[idx]\n        labels = self.labels_list[idx]\n\n        encoding = self.tokenizer(\n            words,\n            is_split_into_words=True,\n            truncation=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_tensors='pt'\n        )\n\n        word_ids = encoding.word_ids(batch_index=0)\n\n        # Assign label to LAST subtoken of each word\n        label_ids = [-100] * len(word_ids)\n        for i in range(len(word_ids) - 1, -1, -1):\n            wid = word_ids[i]\n            if wid is None:\n                continue\n            if i == len(word_ids) - 1 or word_ids[i + 1] != wid:\n                label_ids[i] = LABEL2ID[labels[wid]]\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'labels': torch.tensor(label_ids, dtype=torch.long)\n        }\n\ntrain_dataset = PunctDataset(train_words, train_labels, tokenizer, MAX_LEN)\nval_dataset = PunctDataset(val_words, val_labels, tokenizer, MAX_LEN)\ntest_dataset = PunctDataset(test_words, test_labels, tokenizer, MAX_LEN)\n\nprint(f\"Datasets: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n\nsample = train_dataset[0]\nprint(f\"Input shape: {sample['input_ids'].shape}\")\nprint(f\"Non-ignored labels: {(sample['labels'] != -100).sum().item()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=NUM_LABELS,\n    id2label=ID2LABEL,\n    label2id=LABEL2ID,\n)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n\n    true_labels = []\n    pred_labels = []\n    for i in range(labels.shape[0]):\n        for j in range(labels.shape[1]):\n            if labels[i][j] != -100:\n                true_labels.append(ID2LABEL[labels[i][j]])\n                pred_labels.append(ID2LABEL[preds[i][j]])\n\n    report = classification_report(\n        true_labels, pred_labels,\n        labels=['O', 'COMMA', 'PERIOD', 'QUESTION'],\n        output_dict=True\n    )\n\n    return {\n        'f1_weighted': report['weighted avg']['f1-score'],\n        'precision_weighted': report['weighted avg']['precision'],\n        'recall_weighted': report['weighted avg']['recall'],\n    }\n\ntraining_args = TrainingArguments(\n    output_dir='./xlmr_punct_kyrgyz',\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    load_best_model_at_end=True,\n    metric_for_best_model='f1_weighted',\n    greater_is_better=True,\n    fp16=True,\n    logging_steps=50,\n    report_to='none',\n    seed=42,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\nprint(\"Starting XLM-RoBERTa training...\")\ntrainer.train()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n\nmodel.eval()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nall_true = []\nall_pred = []\n\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels']\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=-1).cpu()\n\n        for i in range(labels.shape[0]):\n            for j in range(labels.shape[1]):\n                if labels[i][j] != -100:\n                    all_true.append(ID2LABEL[labels[i][j].item()])\n                    all_pred.append(ID2LABEL[preds[i][j].item()])\n\nprint(f\"Total test tokens: {len(all_true)}\")\nprint()\nprint(\"=\" * 60)\nprint(\"XLM-RoBERTa CLASSIFICATION REPORT\")\nprint(\"=\" * 60)\nprint(classification_report(\n    all_true, all_pred,\n    labels=['O', 'COMMA', 'PERIOD', 'QUESTION'],\n    digits=3\n))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "report = classification_report(\n    all_true, all_pred,\n    labels=['O', 'COMMA', 'PERIOD', 'QUESTION'],\n    digits=3,\n    output_dict=True\n)\n\nw = report['weighted avg']\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COPY THESE NUMBERS FOR THE PAPER:\")\nprint(\"=\" * 60)\nprint(f\"XLM-RoBERTa (fine-tuned)  &  {w['precision']:.3f}  &  {w['recall']:.3f}  &  {w['f1-score']:.3f}\")\nprint()\nprint(\"Per-class results:\")\nfor cls in ['O', 'COMMA', 'PERIOD', 'QUESTION']:\n    c = report[cls]\n    print(f\"  {cls}: P={c['precision']:.3f}  R={c['recall']:.3f}  F1={c['f1-score']:.3f}  Support={c['support']}\")\nprint()\nprint(\"LaTeX table row:\")\nprint(f\"XLM-RoBERTa (fine-tuned) & {w['precision']:.3f} & {w['recall']:.3f} & {w['f1-score']:.3f} \\\\\\\\\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Confusion Matrix",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nlabels = ['O', 'COMMA', 'PERIOD', 'QUESTION']\n\n# Normalized confusion matrix\ncm_norm = confusion_matrix(all_true, all_pred, labels=labels, normalize='true')\n\nfig, ax = plt.subplots(figsize=(6, 5))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=labels)\ndisp.plot(ax=ax, cmap='Oranges', values_format='.3f')\nax.set_title('XLM-RoBERTa — Confusion Matrix (Normalized)', fontsize=13)\nplt.tight_layout()\nplt.savefig('confusion_matrix_xlmr.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\"Saved: confusion_matrix_xlmr.png\")\n\n# Raw counts\ncm_raw = confusion_matrix(all_true, all_pred, labels=labels)\nprint(\"\\nRaw counts:\")\nprint(cm_raw)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "files.download('confusion_matrix_xlmr.png')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}